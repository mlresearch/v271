---
title: Randomised Postiterations for Calibrated BayesCG
abstract: The Bayesian conjugate gradient method offers probabilistic solutions to
  linear systems but suffers from poor calibration, limiting its utility in uncertainty
  quantification tasks. Recent approaches leveraging postiterations to construct priors
  have improved computational properties but failed to correct calibration issues.
  In this work, we propose a novel randomised postiteration strategy that enhances
  the calibration of the BayesCG posterior while preserving its favourable convergence
  characteristics. We present theoretical guarantees for the improved calibration,
  supported by results on the distribution of posterior errors. Numerical experiments
  demonstrate the efficacy of the method in both synthetic and inverse problem settings,
  showing enhanced uncertainty quantification and better propagation of uncertainties
  through computational pipelines.
year: '2025'
openreview: yKVnv2g39e
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vyas25a
month: 0
tex_title: Randomised Postiterations for Calibrated {B}ayesCG
firstpage: 75
lastpage: 83
page: 75-83
order: 75
cycles: false
bibtex_author: Vyas, Niall and Hegde, Disha and Cockayne, Jon
author:
- given: Niall
  family: Vyas
- given: Disha
  family: Hegde
- given: Jon
  family: Cockayne
date: 2025-08-27
address:
container-title: Proceedings of the First International Conference on Probabilistic
  Numerics
volume: '271'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 8
  - 27
pdf: https://raw.githubusercontent.com/mlresearch/v271/main/assets/vyas25a/vyas25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
